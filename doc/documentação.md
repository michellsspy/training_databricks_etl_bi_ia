# Projeto ETL/ELT Hotelaria - Arquitetura Medallion

Este documento detalha o processo de configura√ß√£o, inicializa√ß√£o e implementa√ß√£o da camada Bronze do Lakehouse, utilizando Databricks Asset Bundles (DABs) e Delta Live Tables (DLT).

## 1. Setup do Ambiente de Desenvolvimento (Linux/Ubuntu)

O ambiente de desenvolvimento foi configurado em uma esta√ß√£o de trabalho Ubuntu, garantindo o isolamento de depend√™ncias e a sincroniza√ß√£o autom√°tica com o Databricks Workspace via CLI.

### 1.1. Atualiza√ß√£o do Sistema e Depend√™ncias Base
Inicialmente, preparamos o sistema operacional com as ferramentas necess√°rias para compila√ß√£o e gerenciamento de rede.

```bash
# Atualiza√ß√£o de pacotes do sistema
sudo apt update && sudo apt upgrade -y

# Instala√ß√£o do Python, gerenciador de pacotes e ferramentas de download
sudo apt install python3-pip python3-venv curl -y

```

### 1.2. Instala√ß√£o da Databricks CLI

A CLI √© a interface fundamental para realizar o deploy dos arquivos de configura√ß√£o (YAML) e dos scripts de transforma√ß√£o para o ambiente de nuvem.

```bash
# Download e instala√ß√£o autom√°tica do bin√°rio databricks
curl -fsSL [https://raw.githubusercontent.com/databricks/setup-cli/main/install.sh](https://raw.githubusercontent.com/databricks/setup-cli/main/install.sh) | sh

# Verifica√ß√£o da integridade da instala√ß√£o
databricks --version

```

### 1.3. Instala√ß√£o e Configura√ß√£o do UV

Para otimizar o processo de build dos artefatos Python (.whl) e evitar erros de execu√ß√£o no pipeline (como o status 127), instalamos o `uv`.

```bash
# Instala√ß√£o do gerenciador de pacotes e build UV
curl -LsSf [https://astral.sh/uv/install.sh](https://astral.sh/uv/install.sh) | sh

# Configura√ß√£o do PATH no terminal (essencial para reconhecimento do comando)
export PATH="$HOME/.local/bin:$PATH"

# Persist√™ncia da configura√ß√£o no perfil do usu√°rio
echo 'export PATH="$HOME/.local/bin:$PATH"' >> ~/.bashrc
source ~/.bashrc

```

### 1.4. Inicializa√ß√£o do Projeto e Ambiente Virtual

Configuramos o reposit√≥rio local com um ambiente virtual isolado para evitar conflitos entre as bibliotecas do projeto e as bibliotecas do sistema.

```bash
# Navega√ß√£o at√© o diret√≥rio raiz do projeto
cd ~/Documentos/Git_Clones/training_databricks_etl_bi_ia/my_project

# Cria√ß√£o do ambiente virtual Python
python3 -m venv venv

# Ativa√ß√£o do ambiente
source venv/bin/activate

# Instala√ß√£o de bibliotecas suporte (Faker para mock e Pytest para testes unit√°rios)
pip install Faker names pytest build

```

### 1.5. Autentica√ß√£o OAuth e Valida√ß√£o de Bundle

A conex√£o entre o ambiente local e o Databricks Workspace √© feita via OAuth, garantindo seguran√ßa sem a exposi√ß√£o de tokens em arquivos de texto.

```bash
# Configura√ß√£o da autentica√ß√£o (dispara abertura do navegador para login)
databricks configure

# Valida√ß√£o t√©cnica do Asset Bundle (verifica databricks.yml e recursos)
databricks bundle validate

```

### 1.6. Primeiros Comandos de Deploy

Com o ambiente configurado, o projeto est√° pronto para ser sincronizado.

```bash
# Envio dos artefatos e configura√ß√µes para o Workspace
databricks bundle deploy

```

# 2. Gera√ß√£o de Dados e Camada Transient

Esta etapa documenta a estrat√©gia para popular o Lakehouse com dados sint√©ticos de alta fidelidade, simulando um sistema de gest√£o hoteleira real.

## 2.1. Estrat√©gia de Mock Data
Utilizamos as bibliotecas `Faker` e `names` para gerar volumetria controlada de entidades relacionadas, garantindo a integridade referencial entre Hoteis, Quartos, Hospedes e Reservas.

## 2.2. Implementa√ß√£o da Gera√ß√£o (Notebook de Setup)
A gera√ß√£o foi executada atrav√©s de um script Spark que distribui a carga de cria√ß√£o e persiste o resultado em tabelas Delta no cat√°logo de desenvolvimento.

### Entidades Geradas
* **Hoteis (source_hoteis):** 50 registros com atributos de categoria, estrelas e localiza√ß√£o.
* **Quartos (source_quartos):** ~5.000 registros vinculados aos hot√©is, com tipos de su√≠te e pre√ßos din√¢micos.
* **Hospedes (source_hospedes):** 8.000 registros com dados demogr√°ficos e programas de fidelidade.
* **Reservas (source_reservas):** 15.000 registros simulando check-ins, check-outs e cancelamentos.
* **Consumos, Faturas e Reservas OTA:** Tabelas dependentes que completam o ciclo financeiro.

## 2.3. Persist√™ncia na Camada Transient
Diferente de um Data Lake tradicional que usa arquivos brutos, optamos por persistir os dados na zona `transient` como tabelas Delta para facilitar o consumo via `readStream` na camada Bronze.

```python
# Comandos SQL utilizados para cria√ß√£o do namespace
CREATE CATALOG IF NOT EXISTS development;
CREATE SCHEMA IF NOT EXISTS development.transient;

# Exemplo de salvamento na Transient (Executado via Notebook)
df_hoteis.write.mode("overwrite").saveAsTable("development.transient.source_hoteis")
df_hospedes.write.mode("overwrite").saveAsTable("development.transient.source_hospedes")

```

## 2.4. Valida√ß√£o da Camada

Ap√≥s a execu√ß√£o do script de gera√ß√£o, a valida√ß√£o √© feita via consulta SQL para garantir que a volumetria condiz com o esperado pelo projeto.

```sql
-- Valida√ß√£o de volumetria na Transient
SELECT 'Hospedes' as Entidade, COUNT(*) as Total FROM development.transient.source_hospedes
UNION ALL
SELECT 'Reservas' as Entidade, COUNT(*) as Total FROM development.transient.source_reservas;

```

# 3. Camada Bronze: Ingest√£o Incremental e Qualidade

Esta etapa documenta a transi√ß√£o dos dados da zona `transient` para a camada `bronze`, onde os dados s√£o persistidos de forma imut√°vel e enriquecidos com metadados de auditoria.

## 3.1. Arquitetura de Ingest√£o (Append-Only)
A camada Bronze foi implementada utilizando **Delta Live Tables (DLT)** com o m√©todo `readStream`. Esta abordagem garante que o pipeline processe apenas novos registros (micro-batches), mantendo a integridade do hist√≥rico e a conformidade com o padr√£o de "Verdade T√©cnica".

## 3.2. Matriz de Metadados de Auditoria
Para cada registro ingerido, adicionamos obrigatoriamente um conjunto de metadados para garantir rastreabilidade e integridade bit-a-bit:
* **_metadata_source_system**: Identifica√ß√£o da origem (ex: hotel_management).
* **_metadata_ingestion_at**: Timestamp exato da carga.
* **_metadata_source_file**: Caminho da tabela de origem na transient.
* **_metadata_row_hash**: Hash SHA256 de todas as colunas para detec√ß√£o de mudan√ßas (Change Detection).
* **_metadata_row_bin**: Representa√ß√£o bin√°ria (UTF-8) da linha para prova de integridade.

## 3.3. Testes de Qualidade (DLT Expectations)
Implementamos regras de sanidade t√©cnica para proteger o Lakehouse contra dados corrompidos ou incompletos:
* **expect_or_fail**: Aborta o pipeline se IDs cr√≠ticos forem nulos.
* **expect_or_drop**: Remove registros com inconsist√™ncias l√≥gicas (ex: Checkout menor que Checkin).
* **expect**: Registra m√©tricas de observabilidade para campos com formatos suspeitos (ex: CPFs curtos).

## 3.4. Exemplo de Implementa√ß√£o T√©cnica
Abaixo, o padr√£o utilizado para todas as 07 entidades do dom√≠nio de hotelaria:

```python
import dlt
from pyspark.sql import functions as F

SISTEMA = "hotel_management"
ENTIDADE = "reservas"
SOURCE_TABLE = "development.transient.source_reservas"

@dlt.table(
    name=f"bronze_{SISTEMA}_{ENTIDADE}",
    comment=f"Tabela Bronze com dados brutos de {ENTIDADE}.",
    table_properties={
        "quality": "bronze",
        "delta.enableChangeDataFeed": "true",
        "pipelines.autoOptimize.zOrderCols": "_metadata_ingestion_at"
    }
)
# --- TESTES DE QUALIDADE (EXPECTATIONS) ---
@dlt.expect_or_fail("reserva_id_valido", "reserva_id IS NOT NULL")
@dlt.expect_or_drop("datas_logicas", "data_checkout >= data_checkin")
@dlt.expect("status_conhecido", "status_reserva IN ('Conclu√≠da', 'Hospedado', 'Confirmada', 'Cancelada')")
def bronze_reservas():
    df_raw = spark.readStream.table(SOURCE_TABLE)
    
    columns_to_concat = [F.coalesce(F.col(c).cast("string"), F.lit("")) for c in df_raw.columns]
    line_concat = F.concat_ws("||", *columns_to_concat)

    return (
        df_raw
        .withColumn("_metadata_source_system", F.lit(SISTEMA))
        .withColumn("_metadata_ingestion_at", F.current_timestamp())
        .withColumn("_metadata_source_file", F.lit(SOURCE_TABLE))
        .withColumn("_metadata_row_hash", F.sha2(line_concat, 256))
        .withColumn("_metadata_row_bin", F.encode(line_concat, "UTF-8"))
    )

```

## 3.5. Orquestra√ß√£o e Deploy

Os scripts s√£o versionados como arquivos `.py` e orquestrados via Databricks Asset Bundles, garantindo que o cat√°logo de destino seja din√¢mico conforme o ambiente (Dev/Prod).

```bash
# Comando para deploy e execu√ß√£o do pipeline de qualidade
databricks bundle deploy
databricks bundle run my_project_etl

```


Parab√©ns, Michel! Chegar nesse n√≠vel de automa√ß√£o ‚Äî com pipelines separados, inje√ß√£o de vari√°veis de ambiente e orquestra√ß√£o via Jobs ‚Äî √© o que diferencia um Engenheiro de Dados iniciante de um **Arquiteto de Solu√ß√µes**.

Aqui est√° a documenta√ß√£o t√©cnica da Camada Silver para o seu reposit√≥rio. Este documento servir√° como a "fonte da verdade" para qualquer pessoa (ou IA) que precise entender o que acontece entre o dado bruto e o dado pronto para o BI.

---

# ü•à Documenta√ß√£o T√©cnica: Camada Silver

## 1. Vis√£o Geral

A Camada Silver do projeto **Hotel Management** √© respons√°vel por transformar os dados brutos (Bronze) em tabelas normalizadas, tipadas e validadas. O principal objetivo √© garantir a consist√™ncia dos dados para o consumo anal√≠tico, aplicando regras de neg√≥cio e versionamento hist√≥rico.

## 2. Arquitetura de Processamento

* **Motor:** Delta Live Tables (DLT).
* **Orquestra√ß√£o:** Databricks Workflows (Job) com depend√™ncia entre camadas.
* **Cat√°logo:** Din√¢mico (Unity Catalog), injetado via Databricks Asset Bundles (DABs).
* **Estrat√©gia de Carga:** * **SCD Tipo 1:** Sobrescrita de registros existentes para manter o estado atual (Hot√©is, Quartos, Reservas, Consumos, Faturas, OTAs).
* **SCD Tipo 2:** Hist√≥rico completo de altera√ß√µes (H√≥spedes).



---

## 3. Matriz de Entidades e Transforma√ß√µes

| Tabela Silver | Origem (Bronze) | Tipo de Hist√≥rico | Principais Transforma√ß√µes |
| --- | --- | --- | --- |
| `silver_hospedes` | `bronze_hospedes` | **SCD Tipo 2** | Normaliza√ß√£o de CPF, Initcap em nomes, Email em lowercase. |
| `silver_hoteis` | `bronze_hoteis` | SCD Tipo 1 | Casting de Latitude/Longitude (Decimal 18,4), Padroniza√ß√£o de Estados. |
| `silver_reservas` | `bronze_reservas` | SCD Tipo 1 | C√°lculo de precis√£o financeira, casting de datas e status. |
| `silver_quartos` | `bronze_quartos` | SCD Tipo 1 | Tipagem booleana (ar-condicionado, fumante) e pre√ßos decimais. |
| `silver_consumos` | `bronze_consumos` | SCD Tipo 1 | Precis√£o de 4 casas decimais em valores de servi√ßos. |
| `silver_faturas` | `bronze_faturas` | SCD Tipo 1 | Valida√ß√£o de data de vencimento e integridade de impostos. |
| `silver_reservas_ota` | `bronze_reservas_ota` | SCD Tipo 1 | Concilia√ß√£o de taxas de comiss√£o e valor l√≠quido. |

---

## 4. Padr√µes de Qualidade (Data Quality)

Implementamos **Expectations** para impedir que dados logicamente incorretos poluam o Data Lakehouse:

* **Integridade Referencial:** IDs mandat√≥rios n√£o nulos.
* **Sanidade Financeira:** Valores totais, di√°rias e consumos devem ser `>= 0`.
* **L√≥gica Temporal:** `data_checkout >= data_checkin` e `data_vencimento >= data_emissao`.
* **Formata√ß√£o:** Emails devem seguir o padr√£o `%@%.%` e CPFs devem ter 11 d√≠gitos.

---

## 5. Estrutura de Inje√ß√£o de Vari√°veis (CI/CD)

Para garantir a portabilidade entre ambientes (Dev/Prod), utilizamos o objeto `spark.conf` para capturar o cat√°logo alvo definido no deploy:

```python
# Exemplo de leitura din√¢mica
current_catalog = spark.conf.get("project.catalog")
source_table = f"{current_catalog}.bronze.bronze_hotel_management_reservas"

```

---

## 6. Como Executar

O deploy e a execu√ß√£o s√£o feitos via terminal utilizando o Databricks CLI:

1. **Validar:** `databricks bundle validate`
2. **Deploy:** `databricks bundle deploy -t dev`
3. **Executar:** `databricks bundle run main_etl_hotel_job`

---

### Registro de Observabilidade Visual

```plaintext
================================================================================
DOC CHECKPOINT | STATUS: DOCUMENTA√á√ÉO CONCLU√çDA | DATA: 2026-02-01
================================================================================
[*] Camada: Silver (Normaliza√ß√£o).
[*] Framework: Markdown + DLT Metadata.
‚îî‚îÄ Status: ‚úÖ PRONTO PARA O REPOSIT√ìRIO.
================================================================================

